# RAG-Knowledge-Explorer

## Project Overview

RAG-Knowledge-Explorer is a Django-based backend designed to demonstrate and run a Retrieval-Augmented Generation (RAG) workflow. It combines document ingestion, vector search, and large language model (LLM) generation so you can build chat and assistant experiences grounded in your own documents. The project was created to explore practical patterns for integrating embeddings, vector stores, and streaming LLM responses in a small, deployable service.

## Why this project exists

- To provide a practical, opinionated reference implementation of a RAG system using modern components (embeddings + vector store + LLM).
- To experiment with streaming responses (SSE) and saving final conversational state.
- To offer a lightweight foundation for prototypes or proof-of-concepts that need search + generative AI over private documents.
- To illustrate how to switch between development (in-memory) and production (pgvector + PostgreSQL) vector stores.

## Main components

- Django project (REST API)
- RAG orchestration layer (RAGSystem): handles embeddings, vector store management, retrievers, and LLM calls
- Document ingestion & chunking pipeline
- Vector store (in-memory for dev; PostgreSQL + pgvector for production)
- Chat endpoints (synchronous and streaming via SSE)
- Session management: each RAG agent/session stores configuration, messages, and metadata
- Simple web API documentation (Swagger / Redoc)

## Technologies used

- Python + Django (REST endpoints)
- Django REST framework (API views/serializers)
- LangGraph (orchestrates LLM + embedding workflows in-code)
- Azure OpenAI (LLM) and Azure embeddings (configurable via env)
- Vector store options:
  - In-memory vector store for development
  - PostgreSQL + pgvector for production deployments
- Server-Sent Events (SSE) for streaming responses
- Redis (optional) for async tasks / Celery (configured via env if used)
- Docker (optional Dockerfile included for container builds)
- Pytest (test scaffolding / management commands included)

## High-level architecture

1. Client uploads documents to a session (DocumentUpload endpoint).
2. Documents are chunked and embedded, then stored in a vector index.
3. Client sends chat queries to the Chat endpoint.
4. The system retrieves relevant passages from the vector store and calls the LLM with the retrieved context and session instructions.
5. Responses are returned either as a single completed message (sync) or streamed back as partial events (SSE).
6. Final assistant messages and conversation history are persisted to the session.

## API Endpoints

Base prefix: /api/

- POST /api/document-upload/  
  Upload documents (PDF, TXT, etc.), create a session, and build the vector index.

- POST /api/chat/  
  Synchronous chat endpoint that accepts a user message and returns a complete assistant reply.

- POST /api/chat/stream/  
  Streaming chat (Server-Sent Events). Returns partial generation chunks as they are produced, and persists the final assistant message when complete.

- GET /api/sessions/  
  List existing RAG sessions.

- GET /api/rag-agents/<uuid:session_id>/  
  Get session details, metadata, and status.

- PUT /api/rag-agents/<uuid:session_id>/update/  
  Update session-level instructions, descriptions, or configuration.

- DELETE /api/rag-agents/<uuid:session_id>/delete/  
  Delete a session and its vector data.

## Environment variables (important)

Copy `.env.example` to `.env` and populate:

- AZURE_API_KEY, AZURE_ENDPOINT, AZURE_DEPLOYMENT — LLM configuration
- EMBED_API_KEY, EMBED_ENDPOINT, EMBED_DEPLOYMENT — Embedding model config
- DATABASE_URL / Django DB settings — PostgreSQL for production
- REDIS_URL — optional (Celery/broker)
- DEBUG — control dev vs prod behavior
- SECRET_KEY — Django secret

## Installation & quick start

1. Clone repository and enter folder:
   - (You're already in c:\Users\USER\Documents\LangGraph-Django)

2. Create and activate a virtual environment:
   - Windows PowerShell:
     ```powershell
     python -m venv .venv
     .\.venv\Scripts\Activate.ps1
     ```

3. Install dependencies:
   ```powershell
   pip install -r requirements.txt
   ```

4. Configure environment:
   - Copy `.env.example` to `.env` and fill required keys.

5. Run migrations:
   ```powershell
   python manage.py migrate
   ```

6. Start the server:
   ```powershell
   python manage.py runserver
   ```
   
## Streaming behavior

The streaming endpoint uses Server-Sent Events (SSE). Partial tokens/blocks generated by the LLM are emitted immediately to clients, improving perceived latency for interactive use. After streaming completes, the assembled assistant message is saved to the session history.
