# RAG System with LangGraph and Django: Research & Analysis

## System Architecture Overview

The system implements a Retrieval-Augmented Generation (RAG) framework using Django as the web framework and LangGraph for orchestrating the language model workflow. The architecture consists of several key components:

1. **Web Interface Layer**: Django-powered RESTful API endpoints that handle client requests and responses
2. **RAG System Core**: A LangGraph-powered state management system that orchestrates the retrieval and generation processes
3. **Document Processing Pipeline**: Handles document upload, chunking, embedding, and vector storage
4. **Streaming Implementation**: Real-time response streaming using Django's StreamingHttpResponse
5. **Database Layer**: Django ORM models for persistent storage of sessions, documents, and chat history
6. **Metadata Management**: Comprehensive tracking of document metadata, chunk information, and source attribution

The system follows a microservices-inspired architecture where different components handle specific responsibilities, improving maintainability and scalability.

## Technology Justifications

### Django Framework
- **Robustness**: Mature web framework with built-in security features
- **ORM System**: Simplifies database operations and migrations
- **REST Framework**: Easy API creation with serialization support
- **Admin Interface**: Built-in admin panel for data management
- **Streaming Support**: Native support for streaming responses

### LangGraph
- **State Management**: Manages complex RAG workflow state transitions
- **Graph-based Flow**: Allows for branching logic in the RAG pipeline
- **Message Passing**: Facilitates communication between nodes in the graph
- **Extensibility**: Easy to add new nodes or modify workflow

### Vector Database
- **Development Mode**: In-memory vector storage for quick development
- **Production Mode**: PostgreSQL with pgvector extension for scalable deployment
- **Similarity Search**: Efficient retrieval of relevant document chunks

### Azure OpenAI Integration
- **API Compatibility**: Works with both Azure OpenAI and standard OpenAI API
- **Embedding Generation**: Uses embedding models for document vectorization
- **LLM Integration**: Supports streaming responses from Azure OpenAI

## Document Retrieval Plan

The document retrieval process follows these steps:

1. **Document Ingestion**:
   - Documents are uploaded through the `DocumentUploadView`
   - Supported formats include TXT, PDF, and DOCX
   - Files are processed and stored in the database

2. **Text Chunking**:
   - Documents are split into manageable chunks using `RecursiveCharacterTextSplitter`
   - Chunk size and overlap are configurable
   - Chunk metadata is stored in the `ChunkMetadata` model

3. **Embedding Generation**:
   - Text chunks are converted to vector embeddings using Azure OpenAI's embedding models
   - Embeddings are stored in the vector database

4. **Similarity Search**:
   - When a user query is received, the system generates an embedding for the query
   - Cosine similarity calculation identifies the most relevant document chunks
   - Top-k matching chunks are retrieved and used as context

5. **Result Handling**:
   - Retrieved chunks are formatted into a context string
   - Source information is preserved for citation

## Chat Memory Design

The chat memory system maintains conversation history and context:

1. **Session Management**:
   - Each user interaction is tied to a `RAGSession` instance
   - Sessions can be created, retrieved, updated, and deleted via API endpoints
   - Sessions store metadata like name, description, and instructions

2. **Message Storage**:
   - Chat messages are stored in the `ChatMessage` model
   - Messages are categorized as user, assistant, or system
   - Each message contains content, timestamp, and source references

3. **Conversation Context**:
   - Previous messages are included in subsequent queries to maintain context
   - The system formats chat history appropriately for the language model
   - Instructions and session description provide additional context

4. **Source Tracking**:
   - The system tracks which document chunks were used to generate responses
   - Source information is stored in the message model for reference
   - This enables attribution and verification of information

## Streaming Implementation

The streaming implementation provides real-time responses to the client:

1. **StreamingHttpResponse Integration**:
   - Django's `StreamingHttpResponse` enables server-sent events (SSE)
   - Client receives partial responses as they're generated by the LLM
   - Connection remains open until the complete response is delivered

2. **Chunk Processing**:
   - Each token from the LLM is streamed as a JSON object
   - Chunks contain partial text, metadata, and completion status
   - Client can update the UI incrementally as chunks arrive

3. **Event Handling**:
   - Server uses the `data:` prefix for SSE protocol compatibility
   - Error handling with graceful degradation
   - Complete responses are reconstructed on both server and client sides

4. **Message Persistence**:
   - The complete response is saved to the database after streaming completes
   - Source attribution is preserved in the final message

## Scalability & Extensibility

The system is designed for scalability and extensibility:

1. **Database Scalability**:
   - PostgreSQL with pgvector supports efficient vector operations at scale
   - Indexes improve query performance
   - Connection pooling handles concurrent requests

2. **Asynchronous Processing**:
   - Redis integration with Celery for background task processing
   - Offloads resource-intensive operations (document processing, embedding generation)
   - Provides task queuing for handling document processing at scale
   - Supports distributed task execution across multiple workers

3. **Modular Design**:
   - Clear separation between document processing, retrieval, and generation
   - New document types can be added by extending the extraction functions
   - Additional LLM providers can be integrated without changing the core architecture

4. **API Extensibility**:
   - The RESTful API design allows for easy addition of new endpoints
   - Swagger documentation provides clear interface specifications
   - Serializers handle data validation and transformation

5. **Configuration Management**:
   - Environment-based configuration using `decouple`
   - Different settings for development and production environments
   - Feature flags for enabling/disabling components

## Metadata Handling

The system implements a comprehensive metadata management approach:

1. **Document Metadata**:
   - Each document stores essential metadata such as:
     - Original filename and file size
     - Document type (TXT, PDF, DOCX)
     - Upload timestamp and processing status
     - Chunk count and vector store references
   - The `Document` model serves as the primary container for document metadata
   - File content is preserved alongside metadata for reference and reprocessing

2. **Chunk Metadata**:
   - The `ChunkMetadata` model stores information about each text chunk:
     - Chunk content and position within the document (chunk_index)
     - Generated summaries for quick reference
     - Auto-generated question-answer pairs for retrieval enhancement
     - JSON metadata field for flexible additional properties
   - Chunks maintain a relationship to their source document
   - Metadata facilitates efficient retrieval and relevance scoring

3. **Session Metadata**:
   - The `RAGSession` model captures interaction context:
     - Session name, title, and timestamps
     - Description and instruction fields for LLM guidance
     - Relationships to associated documents and messages
   - Session metadata influences response generation and retrieval strategy
   - Active/inactive status tracking for resource management

4. **Vector Store Metadata**:
   - The `VectorStore` model tracks embedding collections:
     - Collection name and document count
     - Creation and update timestamps
     - One-to-one relationship with RAG sessions
   - Facilitates vector database management and cleanup

5. **Message Metadata**:
   - The `ChatMessage` model maintains conversation artifacts:
     - Message type (user, assistant, system)
     - Timestamp and content
     - Source attribution via JSON field
   - Sources list tracks which document chunks contributed to responses
   - Enables verification and explainability of AI-generated content

This comprehensive metadata approach enables advanced features like source attribution, relevance tuning, and audit trails throughout the RAG workflow.

## Testing & Validation

The testing strategy includes:

1. **Unit Tests**:
   - Testing individual components in isolation
   - Test coverage for critical functions
   - Mock objects for external dependencies

2. **Integration Tests**:
   - Testing the interaction between components
   - Validating the end-to-end document processing pipeline
   - Testing the streaming implementation with simulated delays

3. **API Tests**:
   - Validating API endpoints and response formats
   - Testing error handling and edge cases
   - Performance testing for response times

4. **Manual Testing**:
   - Testing with various document types and formats
   - Validating retrieval quality with different queries
   - User experience testing for streaming responses

## API Endpoints

### Document Management

1. **Document Upload Endpoint**
   - **URL**: `/rag_api/document-upload/`
   - **Method**: POST
   - **Description**: Uploads multiple documents and creates a new RAG session with user-provided instructions and description
   - **Functionality**:
     - Accepts multiple file uploads (.txt, .pdf, .docx)
     - Processes each document to extract text content
     - For PDF files, uses PyPDF2 to extract text from each page
     - For DOCX files, uses python-docx to extract structured text
     - Chunks the text using RecursiveCharacterTextSplitter for optimal context windows
     - Generates embeddings for each chunk and stores them in the vector database
     - Creates a new RAG session with the provided name and metadata
     - Stores all document metadata and establishes relationships
   - **Parameters**: 
     - `name` (string): Name for the RAG session
     - `description` (string, optional): Context for the RAG responses
     - `instructions` (string, optional): Guidance for the RAG agent
     - `files` (file array): Documents to upload (.txt, .pdf, .docx)
   - **Response**: New RAG session object with document information including chunk counts and processing status
   - **Error Handling**: Provides detailed error messages for file processing failures

2. **File List Endpoint**
   - **URL**: `/rag_api/file-list/`
   - **Method**: GET
   - **Description**: Lists all documents across all sessions with comprehensive metadata
   - **Functionality**:
     - Retrieves all document records from the database
     - Includes file metadata such as size, type, and chunk count
     - Provides relationships to parent sessions
     - Supports optional filtering by document type or session ID
     - Paginates results for large document collections
   - **Response**: Array of document objects with detailed metadata including processing status, chunk counts, and timestamps
   - **Use Case**: Monitoring document corpus and tracking processing status

3. **Add Additional Files Endpoint**
   - **URL**: `/rag_api/add-additional-files/`
   - **Method**: POST
   - **Description**: Adds more documents to an existing session with automatic reprocessing
   - **Functionality**:
     - Verifies the existing session
     - Processes new document uploads similarly to document-upload endpoint
     - Appends new documents to the session's knowledge base
     - Updates the vector store with new embeddings
     - Refreshes session metadata and timestamps
     - Maintains relationships between all documents in the session
   - **Parameters**: 
     - `session_id` (UUID): Existing session ID
     - `files` (file array): Additional documents to upload
   - **Response**: Updated session with new document information and processing status
   - **Use Case**: Expanding the knowledge base of an existing RAG agent with new information

### Session Management

1. **Session List Endpoint**
   - **URL**: `/rag_api/sessions/`
   - **Method**: GET
   - **Description**: Lists all RAG sessions with comprehensive metadata and statistics
   - **Functionality**:
     - Retrieves all active RAG sessions
     - Includes document counts, message counts, and last activity timestamps
     - Provides session metadata including descriptions and instructions
     - Supports filtering by activity status and creation date
     - Sorts results by recent activity by default
   - **Response**: Array of session objects with metadata, document counts, and activity statistics
   - **Use Case**: Managing multiple RAG instances and monitoring system usage

2. **RAG Agent List Endpoint**
   - **URL**: `/rag_api/rag-agents/`
   - **Method**: GET
   - **Description**: Lists all RAG agents (equivalent to sessions) with additional agent-specific metadata
   - **Functionality**:
     - Similar to sessions endpoint but with focus on agent capabilities
     - Includes instruction configurations and description fields
     - Provides document type breakdown for each agent's knowledge base
     - Shows last interaction time and total conversation turns
   - **Response**: Array of RAG agent objects with agent-specific metadata
   - **Use Case**: Administrative overview of all RAG agents in the system

3. **RAG Agent Detail Endpoint**
   - **URL**: `/rag_api/rag-agents/<uuid:session_id>/`
   - **Method**: GET
   - **Description**: Get comprehensive details for a specific RAG agent/session with all related data
   - **Functionality**:
     - Retrieves complete session information by UUID
     - Includes all related documents with their metadata
     - Shows document chunk statistics and processing status
     - Lists recent messages and interactions
     - Provides vector store statistics if available
   - **Response**: Detailed RAG agent object including related documents, message history, and system statistics
   - **Use Case**: In-depth analysis of a specific RAG agent's configuration and knowledge base

4. **RAG Agent Update Endpoint**
   - **URL**: `/rag_api/rag-agents/<uuid:session_id>/update/`
   - **Method**: PUT
   - **Description**: Update RAG agent properties including behavior instructions and metadata
   - **Functionality**:
     - Updates session name, description, and instructions
     - Preserves all document relationships and vector store connections
     - Maintains conversation history while updating agent behavior
     - Updates timestamps to reflect changes
     - Validates input parameters before applying changes
   - **Parameters**:
     - `name` (string, optional): New name
     - `description` (string, optional): New description
     - `instructions` (string, optional): New instructions
   - **Response**: Updated RAG agent object with new properties and timestamp
   - **Use Case**: Refining agent behavior or updating metadata without recreating knowledge base

5. **RAG Agent Delete Endpoint**
   - **URL**: `/rag_api/rag-agents/<uuid:session_id>/delete/`
   - **Method**: DELETE
   - **Description**: Delete a RAG agent/session and all associated resources
   - **Functionality**:
     - Removes the session record from the database
     - Deletes all associated documents and chat messages
     - Cleans up vector store collections if applicable
     - Performs cascade deletion of all related resources
     - Handles potential orphaned resources
   - **Response**: Success confirmation with deletion statistics
   - **Use Case**: Complete removal of a RAG agent and all its associated data

### Chat & Retrieval

1. **Chat Endpoint**
   - **URL**: `/rag_api/chat/`
   - **Method**: POST
   - **Description**: Send a message to a RAG session and get a complete, non-streaming response with source attribution
   - **Functionality**:
     - Validates the session and user message
     - Retrieves previous conversation history for context
     - Saves the user message to the database
     - Processes the message through the RAG pipeline:
       - Generates query embeddings
       - Performs similarity search in the vector database
       - Retrieves relevant document chunks
       - Formats context from retrieved documents
       - Sends context, history, and query to the LLM
       - Processes the complete LLM response
     - Tracks source documents used in the response
     - Saves the assistant's response with source attribution
     - Updates session activity timestamp
   - **Parameters**:
     - `session_id` (UUID): Session ID
     - `message` (string): User message/question
   - **Response**: Assistant response with complete text, source attribution, and metadata
   - **Use Case**: Standard question-answering against the document base

2. **Streaming Chat Endpoint**
   - **URL**: `/rag_api/chat/stream/`
   - **Method**: POST
   - **Description**: Send a message with real-time streaming response using Server-Sent Events
   - **Functionality**:
     - Similar processing pipeline to the standard chat endpoint
     - Establishes a persistent HTTP connection
     - Streams LLM tokens as they're generated using Server-Sent Events (SSE)
     - Returns partial responses in JSON format with the following structure:
       - `token`: The latest generated token
       - `full_response`: The accumulated response so far
       - `sources`: Document references (when available)
       - `done`: Boolean indicating completion status
     - Handles connection drops and client reconnects
     - Saves the complete response to the database once streaming completes
     - Includes detailed error handling with graceful degradation
   - **Parameters**:
     - `session_id` (UUID): Session ID
     - `message` (string): User message/question
   - **Response**: Server-sent events stream with partial responses in real-time
   - **Use Case**: Interactive chat interfaces with immediate feedback

3. **Session Messages Endpoint**
   - **URL**: `/rag_api/session/<uuid:session_id>/messages/`
   - **Method**: GET
   - **Description**: Get the complete conversation history for a specific session
   - **Functionality**:
     - Retrieves all messages for the specified session
     - Returns messages in chronological order
     - Includes message type (user/assistant/system)
     - Provides source attribution for assistant messages
     - Supports pagination for long conversations
     - Optional filtering by message type or date range
   - **Response**: Array of chat messages in chronological order with full metadata
   - **Use Case**: Displaying conversation history, auditing, or exporting conversations

4. **Suggestive Questions Endpoint**
   - **URL**: `/rag_api/suggestive-questions/<uuid:session_id>/`
   - **Method**: GET
   - **Description**: Generate contextually relevant suggested questions based on document content
   - **Functionality**:
     - Analyzes document chunks in the session's knowledge base
     - Extracts key topics and entities from the documents
     - Uses pre-generated question-answer pairs from chunk metadata
     - Dynamically generates relevant questions using the LLM
     - Considers previous conversation history to avoid repetition
     - Returns diverse questions covering different aspects of the knowledge base
   - **Response**: Array of suggested questions with relevance scores and topic categorization
   - **Use Case**: Enhancing user experience by suggesting relevant questions to ask the RAG system

### Demo & Testing

1. **Test API Interface**
   - **URL**: `/rag_api/test/`
   - **Method**: GET
   - **Description**: Interactive HTML interface for testing API endpoints without external tools
   - **Functionality**:
     - Provides a browser-based UI for testing all API endpoints
     - Includes form fields for all required and optional parameters
     - Displays formatted API responses
     - Shows request/response timing and status codes
     - Supports file uploads for document processing endpoints
     - Includes documentation for each endpoint
   - **Response**: HTML page with interactive API test forms and results display
   - **Use Case**: Development testing, demonstration, and API exploration

2. **Streaming Demo**
   - **URL**: `/rag_api/streaming-demo/`
   - **Method**: GET
   - **Description**: Interactive HTML demo showcasing the streaming chat interface capabilities
   - **Functionality**:
     - Provides a complete chat interface with streaming responses
     - Displays token-by-token generation in real-time
     - Shows source documents and attribution
     - Maintains conversation history in the UI
     - Demonstrates optimal streaming implementation
     - Includes controls for selecting different RAG sessions
   - **Response**: HTML page with fully functional streaming chat UI
   - **Use Case**: Demonstrating the streaming capabilities and user experience

## Environment Configuration

The system uses environment variables for configuration, allowing for easy deployment in different environments. A `.env.example` file is provided as a template for setting up the required variables.

### Key Environment Variables

1. **Application Settings**:
   - `DEBUG`: Toggle between development and production modes
   - `SECRET_KEY`: Django's secret key for cryptographic signing
   - `CORS_ALLOWED_ORIGINS`: Comma-separated list of allowed origins for cross-origin requests

2. **Azure OpenAI LLM Settings**:
   - `AZURE_API_KEY`: API key for accessing Azure OpenAI services
   - `AZURE_ENDPOINT`: Endpoint URL for Azure OpenAI
   - `AZURE_API_VERSION`: API version to use
   - `AZURE_DEPLOYMENT`: Deployment name for the LLM model
   - `AZURE_MODEL`: The specific model being used (e.g., gpt-4o)

3. **Azure OpenAI Embedding Settings**:
   - `EMBED_API_KEY`: API key for embedding service
   - `EMBED_ENDPOINT`: Endpoint URL for embedding service
   - `EMBED_API_VERSION`: API version for embeddings
   - `EMBED_DEPLOYMENT`: Deployment name for the embedding model
   - `EMBED_MODEL`: The specific embedding model (e.g., text-embedding-3-large)
   - `EMBED_DIMENSIONS`: The dimension size of the embeddings (e.g., 3072)

4. **Database Settings (for Production)**:
   - `ALLOYDB_SERVER`: PostgreSQL server hostname
   - `ALLOYDB_DATABASE`: Database name
   - `ALLOYDB_USERNAME`: Database username
   - `ALLOYDB_PASS`: Database password

5. **Redis Settings**:
   - `REDIS_URL`: Connection URL for Redis, used for background task queuing with Celery
   
Redis is used in the system for handling asynchronous tasks through Celery, which is particularly useful for:
   - Processing large documents in the background
   - Scheduling periodic tasks such as embedding updates
   - Managing distributed task queues for scalable document processing
   - Caching frequently accessed data to improve response times

To set up the environment:
1. Copy `.env.example` to `.env`
2. Fill in your specific values for each variable
3. Ensure the `.env` file is in the project root directory
4. The application will automatically load these variables using the `python-decouple` library

## Conclusion

The RAG system implemented with Django and LangGraph provides a robust foundation for document-based question answering with streaming capabilities. The architecture balances simplicity with scalability, allowing for future extensions while maintaining performance. The combination of retrieval-augmented generation with real-time streaming responses creates an interactive and responsive user experience with high-quality, sourced information.